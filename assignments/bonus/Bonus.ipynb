{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Completion via Gradient Descent with Probabilistic Perspective (60 pts)"
      ],
      "metadata": {
        "id": "lz4YSC5y_vp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this problem set, we will focus on the matrix completion problem, where we aim to infer the missing entries of a partially observed matrix. Matrix completion has applications in fields like computer vision, signal processing, and recommendation systems.\n",
        "\n",
        "We intent to set up an objective function derived from probabilistic assumptions, evaluate gradients, and apply gradient descent for optimization. Additionally, weâ€™ll explore practical aspects of implementation and validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "UuGDIc-a1N8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "kC_pfeeTY_OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Theoretical Formulation and Objective Properties**"
      ],
      "metadata": {
        "id": "dpOp8G2OBgXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Probabilistic Framework for Matrix Completion (6 pts)**\n",
        "\n",
        "One may interpret matrix completion from a probabilistic standpoint. Let $X \\in \\mathbb{R}^{N \\times M}$ represent our target matrix, where only a subset of entries, indexed by $\\Omega \\subset \\{1, \\ldots, N\\} \\times \\{1, \\ldots, M\\}$, is observed. Denoting the observed entries as $X_{\\Omega}$, we can formulate the posterior probability distribution for $X$ as:\n",
        "\n",
        "$$\n",
        "p(X | X_\\Omega) \\propto p(X_\\Omega | X) p(X),\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $X$ is our estimate of the completed matrix,\n",
        "- $X_\\Omega$ denotes the observed entries of $X$,\n",
        "- $p(X_\\Omega | X)$ is the likelihood of observing $X_\\Omega$ given $X$, and\n",
        "- $p(X)$ is a prior that encourages low-rank solutions."
      ],
      "metadata": {
        "id": "mgr2Mo6dBdvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Low-Rank Matrix Factorization**\n",
        "\n",
        "To simplify optimization, we approximate $X$ by decomposing it into two smaller matrices $U \\in \\mathbb{R}^{N \\times K}$ and $V \\in \\mathbb{R}^{M \\times K}$, where $K$ is an estimate of the rank of $X$ with $K \\ll \\min(N, M)$. We introduce prior distributions for $U$ and $V$ based on normal distributions with precision parameters $\\lambda_U$ and $\\lambda_V$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    p(U | \\lambda_U) &= \\prod_{i = 1}^{N} \\mathcal{N}(\\boldsymbol{u}_i^T | 0, \\lambda_U^{-1} I), \\\\\n",
        "    p(V | \\lambda_V) &= \\prod_{j = 1}^{M} \\mathcal{N}(\\boldsymbol{v}_j^T | 0, \\lambda_V^{-1} I),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{u}_i^T$ and $\\boldsymbol{v}_j^T$ represent the rows of $U$ and $V$, respectively.\n",
        "\n",
        "The observed elements $x_{ij}$ are modeled with precision $\\lambda$ and expected values $\\boldsymbol{u}_i^T \\boldsymbol{v}_j$. Hence, our likelihood function is:\n",
        "\n",
        "$$\n",
        "p(X_\\Omega | U, V, \\lambda) = \\prod_{(i, j) \\in \\Omega} \\mathcal{N}(x_{ij} | \\boldsymbol{u}_i^T \\boldsymbol{v}_j, \\lambda^{-1}).\n",
        "$$\n"
      ],
      "metadata": {
        "id": "4eo9er6tBqP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Objective Function Formulation**\n",
        "\n",
        "Based on this probabilistic setup, we derive the Maximum A Posteriori (MAP) estimate of $U$ and $V$ by minimizing the following objective function:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(U, V) = \\frac{1}{2} \\left\\{ \\lambda \\| \\mathcal{P}_\\Omega(X) - \\mathcal{P}_{\\Omega}(U V^T) \\|_F^2 + \\lambda_U \\| U \\|_F^2 + \\lambda_V \\| V \\|_F^2 \\right\\},\n",
        "$$\n",
        "\n",
        "where $\\mathcal{P}_\\Omega$ denotes a projection operator that zeroes out elements not in $\\Omega$.\n",
        "\n",
        "This formulation shows that MAP estimation for matrix completion reduces to an optimization problem over the factors $U$ and $V$.\n",
        "\n",
        "**<u>Subproblem 1</u> (3 pts):** Derive the MAP objective function $\\mathcal{L}(U, V)$ and explain briefly why minimizing this objective is equivalent to solving the matrix completion problem.\n",
        "\n",
        "**Hint:** You may start from the likelihood and prior expressions, then take the log of the posterior to obtain the objective.\n"
      ],
      "metadata": {
        "id": "LnQBWGQJBvUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Solution to Subproblem 1</u>:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WdLzGbqyByx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4 Properties of the Objective and Regularization**\n",
        "\n",
        "The matrix completion problem can be expressed in terms of finding two low-rank factors $U \\in \\mathbb{R}^{N \\times K}$ and $V \\in \\mathbb{R}^{M \\times K}$ that approximate the observed entries of $X$:\n",
        "\n",
        "$$\n",
        "\\min_{U, V} \\frac{1}{2}\\ \\| \\mathcal{P}_\\Omega(X) - \\mathcal{P}_{\\Omega}(U V^T) \\|_F^2 + \\lambda_U' \\mathcal{R}(U) + \\lambda_V' \\mathcal{R}(V),\n",
        "$$\n",
        "\n",
        "where $\\mathcal{R}$ is a regularization function, and $\\lambda_U' > 0$ and $\\lambda_V' > 0$ are regularization parameters.\n",
        "\n",
        "An alternative approach mentioned in the lecture involves minimizing the **nuclear norm** of $X$, defined as the sum of its singular values, as a proxy for rank minimization:\n",
        "\n",
        "$$\n",
        "\\min_{X} \\frac{1}{2}\\ \\| \\mathcal{P}_\\Omega(X) - \\mathcal{P}_{\\Omega}(X) \\|_F^2 + \\mu \\|X\\|_*,\n",
        "$$\n",
        "\n",
        "where $\\|X\\|_*$ denotes the nuclear norm of $X$, and $\\mu > 0$.\n",
        "\n",
        "**<u>Subproblem 2</u> (3 pts):** Show that this minimzation problem with nuclear norm is equvalent to minimizing defined earlier objective $\\mathcal{L}(U, V)$. How are these setups related to low-rank of $X$? What can you say about the convexity of the objective $\\mathcal{L}(U, V)$?\n",
        "\n",
        "**Hint:** For simplicity, assume $\\lambda_U = \\lambda_V = \\mu$."
      ],
      "metadata": {
        "id": "eQsHYgO_A5Ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Solution to Subproblem 2</u>:**\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6u8VXzxCCJe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Gradient Descent (16 pts)**\n",
        "\n",
        "### **2.1 Gradient Evaluation**\n",
        "\n",
        "Directly solving for $U$ and $V$ through matrix inversion is computationally impractical. Instead, we use **Gradient Descent (GD)** for iterative optimization.\n",
        "\n",
        "**<u>Subproblem 3</u> (3 pts):** Derive the gradients of $\\mathcal{L}(U, V)$ with respect to $U$ and $V$.\n"
      ],
      "metadata": {
        "id": "Ewk4MSM4A_LL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Steepest Gradient Descent and Step Size Calculation**\n",
        "\n",
        "In **Steepest Gradient Descent**, we optimize the step size dynamically for efficient convergence. The optimal step size $\\alpha_U$ for $U$ minimizes the objective along the gradient direction:\n",
        "\n",
        "$$\n",
        "\\alpha_U = \\arg\\min_{\\alpha \\geq 0} \\mathcal{L}(U - \\alpha \\nabla_U \\mathcal{L}, V).\n",
        "$$\n",
        "\n",
        "**<u>Subproblem 4</u> (10 pts):** Derive optimal step sizes $\\alpha_U$ and $\\beta_V$ for updates to $U$ and $V$. These can significantly speed up convergence compared to fixed step sizes."
      ],
      "metadata": {
        "id": "lKOHVizN2xis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Alternating Optimization and Complexity Analysis**\n",
        "\n",
        "In **Alternating Optimization**, we sequentially update $U$ and $V$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    U_{k+1} &= U_k - \\alpha_k \\nabla_U \\mathcal{L}(U_k, V_k), \\\\\n",
        "    V_{k+1} &= V_k - \\beta_k \\nabla_V \\mathcal{L}(U_{k+1}, V_k).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**<u>Subproblem 5</u> (3 pts):** Estimate the computational complexity per iteration in terms of $|\\Omega|$ (observed entries) and $K$ (rank estimate). Identify opportunities to reuse computations from the $U$ update in the $V$ update to improve efficiency."
      ],
      "metadata": {
        "id": "0xlne2BY2z0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Solution to Subproblem 3</u>:**\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TzC4TwLeCoaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Part 3: Implementation and Experiments (20 pts)**\n",
        "\n",
        "To validate our method, we use the relative error on validation and training masks, defined as:\n",
        "\n",
        "$$\n",
        "\\mathrm{RelError} = \\frac{\\|\\mathcal{P}_{\\Omega_{\\text{val}}} \\odot (X - \\hat{X})\\|_F}{\\| \\mathcal{P}_{\\Omega_{\\text{val}}} \\odot X \\|_F }\n",
        "$$\n",
        "\n",
        "where $X$ is the original matrix, $\\hat{X}$ the completion, and $\\mathcal{P}_{\\Omega_{\\text{val}}}$ the validation mask of known elements. The provided function `train_val_masks` splits known elements for you. This error metric is not image-specific but serves our purpose here.\n"
      ],
      "metadata": {
        "id": "-xKcZGo88xp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_masks(X, frac: float=0.8, is_image: bool=False, random_state=0) -> tuple:\n",
        "    \"\"\"Generate training and validation masks for observed data.\"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    observed_target = np.nan_to_num(X) if not is_image else np.copy(X)\n",
        "    mask = (observed_target != 0)\n",
        "    train_mask = ((rng.random(observed_target.shape) < frac) & mask).astype(int)\n",
        "    val_mask = (~train_mask & mask).astype(int)\n",
        "    return train_mask, val_mask, observed_target"
      ],
      "metadata": {
        "id": "FJDhHDuHFCju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataHandler:\n",
        "    \"\"\"Handles image and synthetic data for matrix completion tasks.\"\"\"\n",
        "    def __init__(self, shape=None, rank=None, target_img_path=None, observed_img_path=None, random_state=42):\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "        if shape and rank:\n",
        "            self.target = self._generate_synthetic_data(shape, rank)\n",
        "            self._observed_target = self.target.copy()\n",
        "            self.is_image = False\n",
        "        elif target_img_path and observed_img_path:\n",
        "            self.target = np.array(Image.open(target_img_path)).astype('int')\n",
        "            self._observed_target = np.array(Image.open(observed_img_path)).astype('int')\n",
        "            self.is_image = True\n",
        "        else:\n",
        "            raise ValueError(\"Provide either shape and rank for synthetic data or paths for images.\")\n",
        "        self.N, self.M = self.target.shape\n",
        "\n",
        "    @property\n",
        "    def observed_target(self):\n",
        "        \"\"\"Read-only access to the observed target matrix.\"\"\"\n",
        "        return self._observed_target\n",
        "\n",
        "    def _generate_synthetic_data(self, shape, rank):\n",
        "        \"\"\"Generate a low-rank matrix for synthetic data.\"\"\"\n",
        "        U, V = self.rng.standard_normal((shape[0], rank)), self.rng.standard_normal((shape[1], rank))\n",
        "        return U @ V.T\n",
        "\n",
        "    def apply_missing_entries(self, missed_mode='uniform', frac=0.8, window_shape=None):\n",
        "        \"\"\"Apply missing entries to observed_target.\"\"\"\n",
        "        assert missed_mode in {'uniform', 'window'}, f\"Invalid missed_mode '{missed_mode}'\"\n",
        "        if missed_mode == 'uniform':\n",
        "            R = self.rng.choice(self.target.size, int(self.target.size * frac), replace=False)\n",
        "            self._observed_target.ravel()[R] = np.nan\n",
        "        elif missed_mode == 'window' and window_shape:\n",
        "            n, m = window_shape\n",
        "            if (n > self.N) or (m > self.M):\n",
        "                raise ValueError(\"Submatrix shape is larger than the matrix dimensions.\")\n",
        "            row, col = self.rng.integers(0, self.N - n + 1), self.rng.integers(0, self.M - m + 1)\n",
        "            self._observed_target[row:row + n, col:col + m] = np.nan\n",
        "        return self._observed_target\n",
        "\n",
        "\n",
        "    def visualize(self, scale=0.5):\n",
        "        \"\"\"Display the target and observed matrices/images side by side.\"\"\"\n",
        "        plt.figure(figsize=(15, 4))\n",
        "        grid = plt.GridSpec(1, 2, wspace=0.4)\n",
        "        for i, (data, title) in enumerate(zip([self.target, self._observed_target], ['Target', 'Observed'])):\n",
        "            ax = plt.subplot(grid[0, i])\n",
        "            sns.heatmap(data, ax=ax, xticklabels=int(self.M * scale), yticklabels=int(self.N * scale))\n",
        "            ax.set_title(f'{title} Image') if self.is_image else ax.set_title(f'{title} Matrix')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "bm1bGpnEFY1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Synthetic Data Validation**\n",
        "\n",
        "**<u>Subproblem 6</u>: (10 pts)**\n",
        "- Implement the functions `grad_U` and `grad_V` for computing gradients, `forward` for objective evaluation, `step` method, `relative_err` and `loss` for computing the relative error and objective value respectively.\n",
        "- Run the method on synthetic data `obs_win` and `obs_uni` (they are declared below).\n",
        "- Plot the loss and relative error for each dataset (`obs_win` and `obs_uni`). Compare Gradient Descent (GD) with Steepest Gradient Descent (Steepest GD) and report your findings.\n",
        "\n",
        "**Hint:** If using Python 3.5+ and NumPy 1.10+, you can use the `@` operator to simplify matrix expressions."
      ],
      "metadata": {
        "id": "NcedF1EeDaWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MatrixCompletion:\n",
        "    \"\"\"Performs matrix completion using gradient-based optimization.\"\"\"\n",
        "    def __init__(self, shape, rank, lambd=1e6, lambda_U=1.0, lambda_V=1.0,\n",
        "                 lr_U=0.001, lr_V=0.001, steepest_descent=False, random_state=0):\n",
        "        self.rank = rank\n",
        "        self.lambd_U, self.lambd_V = lambda_U / lambd, lambda_V / lambd\n",
        "        self.lr_U, self.lr_V = lr_U, lr_V\n",
        "        self.steepest_descent = steepest_descent\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "        self.U = self.rng.normal(0.0, 1.0 / lambda_U, size=(shape[0], rank))\n",
        "        self.V = self.rng.normal(0.0, 1.0 / lambda_V, size=(shape[1], rank))\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Compute the matrix approximation U @ V.T.\"\"\"\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    def step(self, X_hat, X, mask, epoch):\n",
        "        \"\"\"Single optimization step using gradient descent.\"\"\"\n",
        "        self.masked_residual = mask * (X - X_hat)\n",
        "        if self.steepest_descent:\n",
        "            self.lr_U = self._lr_steepestGD('U', mask, X)\n",
        "        # Update U factor\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        X_hat = self.forward()\n",
        "        self.masked_residual = mask * (X - X_hat)\n",
        "        if self.steepest_descent:\n",
        "            self.lr_V = self._lr_steepestGD('V', mask, X)\n",
        "        # Update V factor\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss={self.loss(X_hat, X, mask):.4f}\")\n",
        "\n",
        "    def loss(self, X_hat, X, mask):\n",
        "        \"\"\"Compute the loss for the masked matrix.\"\"\"\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    def relative_err(self, X_hat : np.ndarray, X: np.ndarray, mask: np.ndarray) -> float:\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    def _lr_steepestGD(self, param: str, mask: np.ndarray, X) -> float:\n",
        "        \"\"\"Compute the learning rate for steepest descent.\"\"\"\n",
        "        if param == 'U':\n",
        "            return ### YOUR CODE HERE\n",
        "        else:\n",
        "            return ### YOUR CODE HERE\n",
        "\n",
        "    @property\n",
        "    def grad_U(self):\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    @property\n",
        "    def grad_V(self):\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    def show(self, scale: float=0.5, cmap: str=None):\n",
        "        \"\"\"Visualize the output matrix after completion.\"\"\"\n",
        "        X_out = self.forward()\n",
        "        plt.figure(figsize=(10.0, 5.0))\n",
        "        sns.heatmap(X_out, xticklabels=int(X_out.shape[0] * scale),\n",
        "                    yticklabels=int(X_out.shape[1] * scale), cmap=cmap)\n",
        "        title = \"Completed Matrix\" if isinstance(self, MatrixCompletion) else \"Completed Matrix with Side Information\"\n",
        "        plt.title(title)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "4AOe3uAWFgZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, X, train_mask, val_mask, max_epochs):\n",
        "    \"\"\"Train the matrix completion model and track loss/error over epochs.\"\"\"\n",
        "    train_losses, val_losses, train_errs, val_errs = [], [], [], []\n",
        "    for epoch in range(max_epochs):\n",
        "        X_hat = model.forward()\n",
        "        train_loss, train_err = model.loss(X_hat, X, train_mask), model.relative_err(X_hat, X, train_mask)\n",
        "        train_losses.append(train_loss), train_errs.append(train_err)\n",
        "        model.step(X_hat, X, train_mask, epoch)\n",
        "        if val_mask is not None:\n",
        "            val_loss, val_err = model.loss(X_hat, X, val_mask), model.relative_err(X_hat, X, val_mask)\n",
        "            val_losses.append(val_loss), val_errs.append(val_err)\n",
        "    return train_losses, val_losses, train_errs, val_errs"
      ],
      "metadata": {
        "id": "5bK5yOG_FpzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Windowed missing entries\n",
        "matrix_win = DataHandler(shape=(300, 300), rank=5)\n",
        "obs_win = matrix_win.apply_missing_entries(missed_mode='window', window_shape=(50, 50))\n",
        "matrix_win.visualize()\n",
        "\n",
        "mask_train_win, mask_val_win, data_win = train_val_masks(obs_win, frac=0.8)\n",
        "\n",
        "model_win = MatrixCompletion(shape=data_win.shape, rank=5, lambd=1e6, steepest_descent=True)"
      ],
      "metadata": {
        "id": "KrkfDOp8HoF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, train_errs, val_errs = trainer(model_win, data_win, mask_train_win, mask_val_win, max_epochs=200)"
      ],
      "metadata": {
        "id": "oSM4q156HzVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(train_errs, label='Train')\n",
        "plt.semilogy(val_errs, label='Validation')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "jL2OOs1TH06C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uniform missing entries\n",
        "matrix_uni = DataHandler(shape=(300, 300), rank=5)\n",
        "obs_uni = matrix_uni.apply_missing_entries(missed_mode='uniform', frac=0.8)\n",
        "matrix_uni.visualize()\n",
        "\n",
        "mask_train_uni, mask_val_uni, data_uni = train_val_masks(obs_uni, frac=0.8)\n",
        "\n",
        "model_uni = MatrixCompletion(shape=data_uni.shape, rank=5, lambd=1e6, steepest_descent=True)"
      ],
      "metadata": {
        "id": "TpgfSnClH8Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_losses, val_losses, train_errs, val_errs = trainer(model_uni, data_uni, mask_train_uni, mask_val_uni, max_epochs=200)"
      ],
      "metadata": {
        "id": "_1Vnr-ISIBTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(train_errs, label='Train')\n",
        "plt.semilogy(val_errs, label='Validation')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "60GdJWuoICtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Real Data Completion: Cropped Window**\n",
        "\n",
        "**<u>Subproblem 7</u> (5 pts):**\n",
        "- Discuss how to estimate the rank when only part of an image is available.\n",
        "- Test the implementation on `fields_observed.png`.\n",
        "\n",
        "**Hint:** Addressing the question with the rank recall the lecture and seminar on the this topic. There are two straightforward approaches to consider."
      ],
      "metadata": {
        "id": "Txh1MZG8DhbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE FOR ESTIMATING THE RANK OF 'fields_observed.png' BASED ON OBSERVED ENTRIES"
      ],
      "metadata": {
        "id": "WDmvQlnMLBAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_fld = DataHandler(target_img_path='./data/fields.png', observed_img_path='./data/fields_observed.png')\n",
        "obs_fld = image_fld.observed_target\n",
        "image_fld.visualize()\n",
        "\n",
        "mask_train_fld, mask_val_fld, data_fld = train_val_masks(obs_fld, frac=0.8)\n",
        "\n",
        "model_fld = MatrixCompletion(shape=data_fld.shape, rank=50, lambd=1e6, steepest_descent=True)"
      ],
      "metadata": {
        "id": "A_ziwlqDDo2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, train_errs, val_errs = trainer(model_fld, data_fld, mask_train_fld, mask_val_fld, max_epochs=200)"
      ],
      "metadata": {
        "id": "cHAlPsT4IRyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(train_errs, label='Train')\n",
        "plt.semilogy(val_errs, label='Validation')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "qho4MkPvITEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fld.show()"
      ],
      "metadata": {
        "id": "DTbqgM4UIVtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3 Real Data Completion: Uniform Noise**\n",
        "\n",
        "**<u>Subproblem 8</u> (5 pts):** Run the method on `peppers_observed_num_percent.png` images for $\\text{num}=\\{50, 60, 70, 80, 90\\}$, where each image has a different percentage of randomly missing pixels. Estimate the rank as done for the field image."
      ],
      "metadata": {
        "id": "69yc2jZpDl2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE FOR ESTIMATING THE RANK OF 'peppers_observed_num_percent.png' BASED ON OBSERVED ENTRIES\n"
      ],
      "metadata": {
        "id": "ShuDmXO7Lm2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_pep = DataHandler(target_img_path='./data/pepper.png', observed_img_path='./data/peppers_observed_70_percent.png')\n",
        "obs_pep = image_pep.observed_target\n",
        "image_pep.visualize()\n",
        "\n",
        "mask_train_pep, mask_val_pep, data_pep = train_val_masks(obs_pep, frac=0.8)\n",
        "\n",
        "model_pep = MatrixCompletion(shape=data_pep.shape, rank=50, lambd=1e6, steepest_descent=True)"
      ],
      "metadata": {
        "id": "pAWuRNkgDpUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, train_errs, val_errs = trainer(model_pep, data_pep, mask_train_pep, mask_val_pep, max_epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YfMY_SImbc",
        "outputId": "b94d3f67-cf19-4458-aed9-07456653fe8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss=78968424.8359\n",
            "Epoch 20: Train Loss=1965178.4635\n",
            "Epoch 40: Train Loss=935021.5541\n",
            "Epoch 60: Train Loss=599446.7286\n",
            "Epoch 80: Train Loss=434834.2247\n",
            "Epoch 100: Train Loss=337948.3340\n",
            "Epoch 120: Train Loss=274882.1989\n",
            "Epoch 140: Train Loss=231080.5911\n",
            "Epoch 160: Train Loss=199129.2769\n",
            "Epoch 180: Train Loss=174878.8973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(train_errs, label='Train')\n",
        "plt.semilogy(val_errs, label='Validation')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "ZFLoZFkpIoNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pep.show()"
      ],
      "metadata": {
        "id": "2wDof1tdIqCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Bonus - Matrix Completion with Side Information (18 pts)**"
      ],
      "metadata": {
        "id": "3E5cLwbTIz8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 Factorization with Side Information**\n",
        "\n",
        "The effectiveness of the method can sometimes be enhanced by incorporating prior knowledge about the problem. Suppose that we are given that the rows and columns of matrix $X$ lie in lower-dimensional spaces, meaning there exist full-rank matrices\n",
        "$$\n",
        "    G = \\begin{pmatrix} \\boldsymbol{g}_1^T \\\\ \\vdots \\\\ \\boldsymbol{g}_{N}^T \\end{pmatrix} \\in \\mathbb{R}^{N \\times n}, \\quad H = \\begin{pmatrix} \\boldsymbol{h}_1^T \\\\ \\vdots \\\\ \\boldsymbol{h}_{M}^T \\end{pmatrix} \\in \\mathbb{R}^{M \\times m},\n",
        "$$\n",
        "where  $n < N$ and $m < M$, such that\n",
        "\\begin{align*}\n",
        "    \\mathrm{col}(X) &\\subseteq \\mathrm{col}(G) \\\\\n",
        "    \\mathrm{col}(X^T) &\\subseteq \\mathrm{col}(H).\n",
        "\\end{align*}\n",
        "\n",
        "Under this assumption, one can express $X$ as:\n",
        "\n",
        "$$\n",
        "    X = G U V^T H^T,\n",
        "$$\n",
        "\n",
        "where $U \\in \\mathbb{R}^{n \\times K}$ and $V \\in \\mathbb{R}^{m \\times K}$ with $\\text{rank}(X) \\leq K$. We model this as:\n",
        "\n",
        "$$\n",
        "    \\begin{aligned}\n",
        "        p(X_\\Omega | U, V, \\lambda, \\lambda_U, \\lambda_V) &= \\prod\\limits_{(i, j) \\in \\Omega} \\mathcal{N}(x_{i j} | \\boldsymbol{g}_{i}^T U V^T \\boldsymbol{h}_{j}, \\lambda^{-1}) \\\\\n",
        "        p(U | \\lambda_U) &= \\prod\\limits_{l = 1}^{n} \\mathcal{N}(\\boldsymbol{u}_{l}^T | 0, \\lambda_U^{-1} I) \\\\\n",
        "        p(V | \\lambda_V) &= \\prod\\limits_{s = 1}^{m} \\mathcal{N}(\\boldsymbol{v}_{s}^T | 0, \\lambda_V^{-1} I).\n",
        "    \\end{aligned}\n",
        "$$\n",
        "\n",
        "Then, under the new assumptions, the posterior distribution is\n",
        "$$\n",
        "    p(U, V | X_\\Omega, \\lambda, \\lambda_U, \\lambda_V) \\propto p(X_\\Omega | U, V, \\lambda\n",
        " \\, p(U | \\lambda_U) \\, p(V | \\lambda_V).\n",
        "$$\n",
        "In matrix form:\n",
        "$$\n",
        "    \\mathcal{L}_{\\text{SI}}(U, V) = \\frac{1}{2} \\left\\{ \\lambda \\| \\mathcal{P}_\\Omega(X) - \\mathcal{P}_\\Omega(G U V^T H^T) \\|_F^2 + \\lambda_U \\| U \\|_F^2 + \\lambda_V \\| V \\|_F^2 \\right\\}.\n",
        "$$\n",
        "\n",
        "We seek the MAP estimate:\n",
        "$$\n",
        "    \\mathcal{L}_{\\text{SI}}(U, V) \\rightarrow \\min_{U, V} \\quad \\text{s.t.} \\quad U \\in \\mathbb{R}^{n \\times K}, \\, V \\in \\mathbb{R}^{m \\times K}.\n",
        "$$\n",
        "\n",
        "\n",
        "We aim to minimize $\\mathcal{L}_{\\text{SI}}(U, V)$ over $U$ and $V$ using similar gradient descent updates with the matrices $G$ and $H$."
      ],
      "metadata": {
        "id": "8uPciC-19Gw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_side_info(si_dic: str = './data/', img_name: str = 'fields'):\n",
        "    \"\"\"Load precomputed matrices G and H for side information.\"\"\"\n",
        "    return (\n",
        "        np.load(f'{si_dic}G_matrix_{img_name}.npy'),\n",
        "        np.load(f'{si_dic}H_matrix_{img_name}.npy')\n",
        "    )"
      ],
      "metadata": {
        "id": "cvP2GCD8Jjkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Subproblem 9</u> (3 pts):**\n",
        "Compute gradients of $\\mathcal{L}_{\\text{SI}}$ with respect to $U$ and $V$."
      ],
      "metadata": {
        "id": "he4o4FilI5kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Solution to Subproblem 9</u>:**\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "3XgpTwrbJHp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steepest descent for each factor can also be employed here.\n",
        "\n",
        "**<u>Subproblem 10</u> (5 pts):**\n",
        "Derive the step sizes $\\alpha_U$ and $\\beta_V$ for $U$ and $V$ in steepest descent."
      ],
      "metadata": {
        "id": "RzSqBQMxI5am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Solution to Subproblem 10</u>:**\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PAtxl3l4I_lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Subproblem 11</u> (10 pts):**\n",
        "Repeat experiments on real images now applying the method with side information."
      ],
      "metadata": {
        "id": "CqhumohcI_cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MatrixCompletionSI(MatrixCompletion):\n",
        "    \"\"\"Matrix Completion with Side Information Matrices G and H.\"\"\"\n",
        "    def __init__(self, G, H, rank, **kwargs):\n",
        "        shape = (G.shape[1], H.shape[1])\n",
        "        super().__init__(shape=shape, rank=rank, **kwargs)\n",
        "        self.G, self.H = G, H\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Compute matrix approximation with side information.\"\"\"\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    @property\n",
        "    def grad_U(self):\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    @property\n",
        "    def grad_V(self):\n",
        "        return ### YOUR CODE HERE\n",
        "\n",
        "    def _lr_steepestGD(self, param: str, mask: np.ndarray, X) -> float:\n",
        "        if param == 'U':\n",
        "            return ### YOUR CODE HERE\n",
        "        else:\n",
        "            return ### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "T6dDSFJEJgfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image: fields\n",
        "\n",
        "G_fld, H_fld = load_side_info(img_name='fields')\n",
        "\n",
        "mask_train_fld, mask_val_fld, data_fld = train_val_masks(obs_fld, frac=0.8)\n",
        "\n",
        "model_fld_si = MatrixCompletionSI(G_fld, H_fld, rank=r, lambd=1e6, steepest_descent=True)"
      ],
      "metadata": {
        "id": "751OlDPcIuMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_si, val_losses_si, train_errs_si, val_errs_si = trainer(model_fld_si, data_fld, mask_train_fld, mask_val_fld, max_epochs=200)"
      ],
      "metadata": {
        "id": "djs6xUTPIuJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(train_errs_si, label='Train')\n",
        "plt.semilogy(val_errs_si, label='Validation')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "NrMzkbEIIuHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fld_si.show()"
      ],
      "metadata": {
        "id": "xoC2rxkYIuEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z40bobSbKb0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image: pepper\n",
        "\n",
        "G_pep, H_pep = load_side_info(img_name='pepper')\n",
        "\n",
        "mask_train_pep_si, mask_val_pep_si, data_pep = train_val_masks(obs_pep, frac=0.8)\n",
        "\n",
        "model_pep_si = MatrixCompletionSI(G_pep, H_pep, rank=r, lambd=1e6, steepest_descent=True)"
      ],
      "metadata": {
        "id": "LTqbENmbKbx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses_si, val_losses_si, train_errs_si, val_errs_si = trainer(model_pep_si, data_pep, mask_train_pep_si, mask_val_pep_si, max_epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZFhZWeoIuB7",
        "outputId": "4a0889b0-d5fb-4915-99f6-b9676f8b64d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss=70450474.6590\n",
            "Epoch 20: Train Loss=1096290.7651\n",
            "Epoch 40: Train Loss=708811.5183\n",
            "Epoch 60: Train Loss=597904.8178\n",
            "Epoch 80: Train Loss=545734.1375\n",
            "Epoch 100: Train Loss=514562.4336\n",
            "Epoch 120: Train Loss=493036.6314\n",
            "Epoch 140: Train Loss=476694.5734\n",
            "Epoch 160: Train Loss=463518.7184\n",
            "Epoch 180: Train Loss=452486.2511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(train_errs_si, label='Train')\n",
        "plt.semilogy(val_errs_si, label='Validation')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "42JyG6Y6KrzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pep_si.show()"
      ],
      "metadata": {
        "id": "ibXu-BI9KrwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}